# í”„ë¡¬í”„íŠ¸ í‰ê°€ ê°€ì´ë“œ

## í‰ê°€ ì°¨ì›

| ì°¨ì› | ì„¤ëª… | ì¸¡ì • ë°©ë²• |
|------|------|-----------|
| Relevance | ì§ˆë¬¸ì— ëŒ€í•œ ì í•©ì„± | Phase 1 Citation |
| Completeness | í•„ìˆ˜ ì •ë³´ í¬í•¨ | Phase 2 Nugget |
| Groundedness | ê·¼ê±° ê¸°ë°˜ ì—¬ë¶€ | Phase 3 Claim ê²€ì¦ |
| Fluency | ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ | ì‚¬ëŒ í‰ê°€ |
| Consistency | ìºë¦­í„° ì¼ê´€ì„± | í†¤/ìŠ¤íƒ€ì¼ ê²€ì‚¬ |

---

## ìë™ í‰ê°€ ë©”íŠ¸ë¦­

### 1. ê¸¸ì´ ê¸°ë°˜

```python
def evaluate_length(answer: str) -> dict:
    """ê¸¸ì´ ê¸°ë°˜ í‰ê°€"""
    words = len(answer.split())
    sentences = answer.count('.') + answer.count('!') + answer.count('?')

    return {
        "word_count": words,
        "sentence_count": sentences,
        "is_concise": sentences <= 3,  # 3ë¬¸ì¥ ì´ë‚´
        "score": 1.0 if sentences <= 3 else max(0, 1 - (sentences - 3) * 0.2),
    }
```

### 2. í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€

```python
def evaluate_coverage(
    answer: str,
    required_keywords: list[str],
) -> dict:
    """í•„ìˆ˜ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€"""
    found = [kw for kw in required_keywords if kw in answer]
    missing = [kw for kw in required_keywords if kw not in answer]

    return {
        "found_keywords": found,
        "missing_keywords": missing,
        "coverage": len(found) / len(required_keywords) if required_keywords else 1.0,
    }
```

### 3. ìºë¦­í„° ì¼ê´€ì„±

```python
def evaluate_character_consistency(answer: str) -> dict:
    """ìºë¦­í„° ì¼ê´€ì„± ê²€ì‚¬"""
    checks = {
        "uses_honorific": any(h in answer for h in ["ìš”", "ìŠµë‹ˆë‹¤", "ì„¸ìš”"]),
        "has_emoji": any(c in answer for c in "ğŸŒ¿â™»ï¸ğŸ“¦ğŸ”"),
        "is_encouraging": any(e in answer for e in ["ë©‹ì ¸ìš”", "ì¢‹ì•„ìš”", "ì™„ë²½", "í›Œë¥­"]),
        "avoids_negative": not any(n in answer for n in ["ì•ˆë©ë‹ˆë‹¤", "í‹€ë ¸", "ì˜ëª»"]),
    }

    score = sum(checks.values()) / len(checks)

    return {
        "checks": checks,
        "consistency_score": score,
    }
```

---

## LLM ê¸°ë°˜ í‰ê°€

### Relevance í‰ê°€

```python
RELEVANCE_EVAL_PROMPT = """
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„±ì„ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€: {answer}

í‰ê°€ ê¸°ì¤€:
5: ì§ˆë¬¸ì— ì™„ë²½íˆ ë‹µë³€
4: ëŒ€ë¶€ë¶„ ë‹µë³€, ì•½ê°„ì˜ ì¶”ê°€ ì •ë³´ í¬í•¨
3: ë¶€ë¶„ì ìœ¼ë¡œ ë‹µë³€
2: ê´€ë ¨ì€ ìˆìœ¼ë‚˜ í•µì‹¬ ëˆ„ë½
1: ì „í˜€ ê´€ë ¨ ì—†ìŒ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"score": <1-5>, "reason": "<í‰ê°€ ì´ìœ >"}}
"""

async def evaluate_relevance(
    question: str,
    answer: str,
    llm: LLMClientPort,
) -> dict:
    prompt = RELEVANCE_EVAL_PROMPT.format(
        question=question,
        answer=answer,
    )
    response = await llm.generate(prompt)
    return json.loads(response)
```

### Groundedness í‰ê°€

```python
GROUNDEDNESS_EVAL_PROMPT = """
ë‹µë³€ì˜ ê° ì£¼ì¥ì´ ì†ŒìŠ¤ì— ê·¼ê±°í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ì†ŒìŠ¤:
{source}

ë‹µë³€:
{answer}

ê° ì£¼ì¥ì— ëŒ€í•´:
- "supported": ì†ŒìŠ¤ì— ê·¼ê±°í•¨
- "unsupported": ì†ŒìŠ¤ì— ì—†ëŠ” ë‚´ìš©
- "partially": ë¶€ë¶„ì ìœ¼ë¡œ ê·¼ê±°

JSON í˜•ì‹:
{{"claims": [{{"text": "<ì£¼ì¥>", "status": "<ìƒíƒœ>", "source_ref": "<ê·¼ê±° ìœ„ì¹˜>"}}]}}
"""
```

---

## A/B í…ŒìŠ¤íŠ¸

### í”„ë¡¬í”„íŠ¸ ë²„ì „ ë¹„êµ

```python
@dataclass
class PromptVariant:
    name: str
    template: str
    version: str

async def ab_test(
    variants: list[PromptVariant],
    test_cases: list[dict],
    llm: LLMClientPort,
) -> dict:
    """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    results = {v.name: [] for v in variants}

    for case in test_cases:
        for variant in variants:
            prompt = variant.template.format(**case["context"])
            answer = await llm.generate(prompt)

            # í‰ê°€
            eval_result = {
                "relevance": await evaluate_relevance(case["question"], answer, llm),
                "length": evaluate_length(answer),
                "coverage": evaluate_coverage(answer, case.get("keywords", [])),
                "consistency": evaluate_character_consistency(answer),
            }

            results[variant.name].append({
                "case_id": case["id"],
                "answer": answer,
                "evaluation": eval_result,
            })

    # ì§‘ê³„
    summary = {}
    for name, variant_results in results.items():
        scores = [r["evaluation"]["relevance"]["score"] for r in variant_results]
        summary[name] = {
            "avg_relevance": sum(scores) / len(scores),
            "sample_count": len(variant_results),
        }

    return {"results": results, "summary": summary}
```

---

## í‰ê°€ ë°ì´í„°ì…‹

### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ êµ¬ì¡°

```python
TEST_CASES = [
    {
        "id": "waste_001",
        "intent": "waste_query",
        "question": "í˜íŠ¸ë³‘ ì–´ë–»ê²Œ ë²„ë ¤ìš”?",
        "context": {
            "disposal_rules": {...},
            "situation_tags": ["ë¼ë²¨_ë¶€ì°©"],
        },
        "keywords": ["íˆ¬ëª…", "í˜íŠ¸", "ë¼ë²¨"],
        "expected_elements": ["ë°°ì¶œ ì¥ì†Œ", "ë¼ë²¨ ì œê±° ì•ˆë‚´"],
    },
    {
        "id": "general_001",
        "intent": "general",
        "question": "ì•ˆë…•í•˜ì„¸ìš”",
        "context": {},
        "keywords": [],
        "expected_elements": ["ì¸ì‚¬ ì‘ëŒ€"],
    },
]
```

### ê³¨ë“  ë°ì´í„°ì…‹

```python
GOLDEN_ANSWERS = {
    "waste_001": {
        "question": "í˜íŠ¸ë³‘ ì–´ë–»ê²Œ ë²„ë ¤ìš”?",
        "golden_answer": "í˜íŠ¸ë³‘ì€ **íˆ¬ëª… í˜íŠ¸ë³‘ ì „ìš©í•¨**ì— ë²„ë ¤ì£¼ì„¸ìš”! ğŸŒ¿\në¼ë²¨ ë–¼ê³ , ë¹„ìš°ê³ , ì°Œê·¸ëŸ¬ëœ¨ë ¤ì„œ ë°°ì¶œí•˜ì„¸ìš”.",
        "score_threshold": 0.8,
    },
}

async def evaluate_against_golden(
    answer: str,
    golden: str,
    llm: LLMClientPort,
) -> float:
    """ê³¨ë“  ë‹µë³€ ëŒ€ë¹„ í‰ê°€"""
    prompt = f"""
    ë‹¤ìŒ ë‘ ë‹µë³€ì˜ ìœ ì‚¬ë„ë¥¼ 0-1 ì‚¬ì´ë¡œ í‰ê°€í•˜ì„¸ìš”.

    ê³¨ë“  ë‹µë³€: {golden}
    ìƒì„± ë‹µë³€: {answer}

    ìˆ«ìë§Œ ì‘ë‹µí•˜ì„¸ìš”.
    """
    score = float(await llm.generate(prompt))
    return score
```

---

## í‰ê°€ ë¦¬í¬íŠ¸

```python
def generate_eval_report(results: dict) -> str:
    """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = []
    report.append("# í”„ë¡¬í”„íŠ¸ í‰ê°€ ë¦¬í¬íŠ¸\n")

    for variant, data in results["summary"].items():
        report.append(f"## {variant}")
        report.append(f"- í‰ê·  ê´€ë ¨ì„±: {data['avg_relevance']:.2f}")
        report.append(f"- í…ŒìŠ¤íŠ¸ ìˆ˜: {data['sample_count']}")
        report.append("")

    # ìƒì„¸ ê²°ê³¼
    report.append("## ìƒì„¸ ê²°ê³¼\n")
    for variant, variant_results in results["results"].items():
        report.append(f"### {variant}")
        for r in variant_results[:5]:  # ìƒìœ„ 5ê°œ
            report.append(f"- Case {r['case_id']}: {r['evaluation']['relevance']['score']}/5")
        report.append("")

    return "\n".join(report)
```
