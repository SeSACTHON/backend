# ì´ì½”ì—ì½”(EcoÂ²) Agent #16: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (LangGraph 1.0.6)

> LangGraph 1.0+ Message History Management & SummarizationNode íŒ¨í„´

| í•­ëª© | ê°’ |
|-----|-----|
| **ì‘ì„±ì¼** | 2026-01-16 |
| **ë²„ì „** | v1.0 |
| **ì‹œë¦¬ì¦ˆ** | EcoÂ² Agent ì‹œë¦¬ì¦ˆ |
| **ì´ì „ í¬ìŠ¤íŒ…** | [#15 Eval Agent ê³ ë„í™”](./29-chat-eval-agent-enhancement.md) |
| **ì°¸ì¡°** | LangGraph How-to: Manage Message History, langmem |

---

## 1. ë°°ê²½: ë©€í‹°í„´ ëŒ€í™”ì˜ í† í° í•œê³„

### 1.1 ë¬¸ì œ ìƒí™©

```
[Turn 1] ì‚¬ìš©ì: í˜íŠ¸ë³‘ ì–´ë–»ê²Œ ë²„ë ¤?
         AI: í˜íŠ¸ë³‘ì€ ë‚´ìš©ë¬¼ì„ ë¹„ìš°ê³ ...

[Turn 2] ì‚¬ìš©ì: ë¼ë²¨ì€ìš”?
         AI: ë¼ë²¨ì€ ë¶„ë¦¬í•´ì„œ...

[Turn 3] ì‚¬ìš©ì: ëšœê»‘ì€ ì–´ë–»ê²Œ?
         AI: ëšœê»‘ì€...

...

[Turn 20] ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼! ğŸ”¥
```

| ë¬¸ì œ | ì„¤ëª… |
|------|------|
| **í† í° í­ë°œ** | ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ë©”ì‹œì§€ ëˆ„ì  â†’ LLM ì…ë ¥ í•œê³„ ì´ˆê³¼ |
| **ë¹„ìš© ì¦ê°€** | ë§¤ í„´ë§ˆë‹¤ ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ì†¡ â†’ í† í° ë¹„ìš© ê¸‰ì¦ |
| **ì‘ë‹µ ì§€ì—°** | ê¸´ ì»¨í…ìŠ¤íŠ¸ â†’ ì‘ë‹µ ìƒì„± ì‹œê°„ ì¦ê°€ |
| **ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤** | ë‹¨ìˆœ truncation â†’ ì¤‘ìš” ì •ë³´ ìœ ì‹¤ |

### 1.2 LangGraph 1.0+ ì†”ë£¨ì…˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Context Compression Strategy                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [messages] â”€â”€â–¶ token_count > threshold? â”€â”€â–¶ [summarize]        â”‚
â”‚                      â”‚ no                        â”‚               â”‚
â”‚                      â–¼                           â–¼               â”‚
â”‚              [pass-through]            [summary + recent_msgs]   â”‚
â”‚                      â”‚                           â”‚               â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                  â”‚                               â”‚
â”‚                                  â–¼                               â”‚
â”‚                             [LLM input]                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ì•„í‚¤í…ì²˜ ì„¤ê³„

### 2.1 ê°œì„ ëœ íŒŒì´í”„ë¼ì¸

```
START â†’ intent â†’ [vision?] â†’ router
                              â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼          â–¼          â–¼           â–¼           â–¼
                waste    character   location   web_search   general
                (RAG)    (gRPC)      (gRPC)    (DuckDuckGo)  (passthrough)
                   â”‚          â”‚          â”‚           â”‚           â”‚
                   â–¼          â”‚          â”‚           â”‚           â”‚
              [feedback]      â”‚          â”‚           â”‚           â”‚
                   â”‚          â”‚          â”‚           â”‚           â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                   [summarize?] â† NEW: Context Compression
                                         â”‚
                                         â–¼
                                      answer â†’ END
```

### 2.2 ChatState TypedDict

```python
# infrastructure/orchestration/langgraph/state.py

class ChatState(TypedDict, total=False):
    """Chat íŒŒì´í”„ë¼ì¸ ìƒíƒœ - LangGraph 1.0+ ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì§€ì›."""

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ (Reducerë¡œ ëˆ„ì )
    messages: Annotated[list[AnyMessage], add_messages]

    # ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
    summary: str  # ì´ì „ ëŒ€í™” ìš”ì•½
    context: dict[str, Any]  # LLM ì…ë ¥ìš© ì»¨í…ìŠ¤íŠ¸

    # í˜„ì¬ í„´ ì…ë ¥
    query: str
    image_url: str | None

    # íŒŒì´í”„ë¼ì¸ ì¤‘ê°„ ê²°ê³¼
    intent: str
    evidence: list[dict[str, Any]]
    character: dict[str, Any] | None
    location: dict[str, Any] | None
    web_results: list[dict[str, Any]]
    classification_result: str | None

    # í’ˆì§ˆ í‰ê°€
    feedback: dict[str, Any]
    fallback_reason: str | None

    # ìµœì¢… ì¶œë ¥
    answer: str

    # ë©”íƒ€ë°ì´í„°
    job_id: str
    user_id: str
    thread_id: str
```

### 2.3 ë©”ì‹œì§€ Reducer

```python
def add_messages(
    existing: list[AnyMessage] | None,
    new: list[AnyMessage] | AnyMessage,
) -> list[AnyMessage]:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ë³‘í•© Reducer.

    LangGraph 1.0+ Annotated íŒ¨í„´ ì‚¬ìš©.
    ë§¤ í„´ë§ˆë‹¤ ë©”ì‹œì§€ê°€ ìë™ìœ¼ë¡œ ëˆ„ì ë¨.
    """
    if existing is None:
        existing = []

    if isinstance(new, list):
        return existing + new
    return existing + [new]
```

---

## 3. SummarizationNode êµ¬í˜„

### 3.1 í† í° ì¹´ìš´íŒ…

```python
# infrastructure/orchestration/langgraph/summarization.py

DEFAULT_MAX_TOKENS_BEFORE_SUMMARY = 3072  # ìš”ì•½ íŠ¸ë¦¬ê±° ì„ê³„ê°’
DEFAULT_MAX_SUMMARY_TOKENS = 512  # ìš”ì•½ ìµœëŒ€ í† í°
DEFAULT_KEEP_RECENT_MESSAGES = 4  # ìµœê·¼ Nê°œ ë©”ì‹œì§€ ìœ ì§€


def count_tokens_approximately(messages: list["AnyMessage"]) -> int:
    """ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚°.

    ì •í™•í•œ ê³„ì‚°ì€ tiktoken í•„ìš”.
    ì—¬ê¸°ì„œëŠ” ë¬¸ì ìˆ˜ ê¸°ë°˜ ê·¼ì‚¬ì¹˜ ì‚¬ìš© (~4ì = 1í† í°).
    """
    total_chars = 0
    for msg in messages:
        if hasattr(msg, "content"):
            content = msg.content
            if isinstance(content, str):
                total_chars += len(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and "text" in item:
                        total_chars += len(item["text"])
    return total_chars // 4
```

### 3.2 ìš”ì•½ í”„ë¡¬í”„íŠ¸ (PromptLoaderPort í™œìš©)

```
# infrastructure/assets/prompts/summarization/context_compress.txt

ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
í•µì‹¬ ì •ë³´ì™€ ë§¥ë½ë§Œ ìœ ì§€í•˜ê³ , {max_summary_tokens} í† í° ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

{existing_summary_section}ìƒˆë¡œìš´ ëŒ€í™”:
{messages_text}

ìš”ì•½:
```

### 3.3 summarize_messages í•¨ìˆ˜

```python
async def summarize_messages(
    messages: list["AnyMessage"],
    llm: "LLMClientPort",
    existing_summary: str | None = None,
    max_summary_tokens: int = DEFAULT_MAX_SUMMARY_TOKENS,
    prompt_loader: "PromptLoaderPort | None" = None,
) -> str:
    """ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ìš”ì•½.

    Clean Architecture ì¤€ìˆ˜:
    - LLMClientPort: Application Layer Port
    - PromptLoaderPort: Application Layer Port
    """
    if not messages:
        return existing_summary or ""

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (PromptLoader ë˜ëŠ” ê¸°ë³¸ê°’)
    if prompt_loader is not None:
        template = prompt_loader.load_or_default(
            category="summarization",
            name="context_compress",
            default=DEFAULT_SUMMARIZATION_PROMPT,
        )
    else:
        template = DEFAULT_SUMMARIZATION_PROMPT

    # ë©”ì‹œì§€ í…ìŠ¤íŠ¸ êµ¬ì„±
    messages_text = "\n".join(
        f"{msg.__class__.__name__}: {msg.content}"
        for msg in messages
        if hasattr(msg, "content")
    )

    # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
    existing_summary_section = (
        f"ì´ì „ ìš”ì•½:\n{existing_summary}\n\n" if existing_summary else ""
    )
    summary_prompt = template.format(
        max_summary_tokens=max_summary_tokens,
        existing_summary_section=existing_summary_section,
        messages_text=messages_text,
    )

    try:
        summary = await llm.generate(summary_prompt)
        logger.info(
            "conversation_summarized",
            extra={
                "input_messages": len(messages),
                "summary_length": len(summary),
            },
        )
        return summary
    except (ValueError, RuntimeError) as e:
        logger.error(
            "summarization_failed",
            extra={"error": str(e), "error_type": type(e).__name__},
        )
        return existing_summary or ""
```

### 3.4 SummarizationNode í´ë˜ìŠ¤

```python
class SummarizationNode:
    """LangGraph ë…¸ë“œìš© ìš”ì•½ í´ë˜ìŠ¤.

    langmemì˜ SummarizationNodeì™€ ìœ ì‚¬í•œ ì¸í„°í˜ì´ìŠ¤.
    ë…ë¦½ ë…¸ë“œë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ pre_model_hookìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥.
    """

    def __init__(
        self,
        llm: "LLMClientPort",
        token_counter: Callable[[list["AnyMessage"]], int] = count_tokens_approximately,
        max_tokens_before_summary: int = DEFAULT_MAX_TOKENS_BEFORE_SUMMARY,
        max_summary_tokens: int = DEFAULT_MAX_SUMMARY_TOKENS,
        keep_recent_messages: int = DEFAULT_KEEP_RECENT_MESSAGES,
        prompt_loader: "PromptLoaderPort | None" = None,
    ):
        self.llm = llm
        self.token_counter = token_counter
        self.max_tokens_before_summary = max_tokens_before_summary
        self.max_summary_tokens = max_summary_tokens
        self.keep_recent_messages = keep_recent_messages
        self.prompt_loader = prompt_loader

        self._hook = create_summarization_hook(
            llm=llm,
            token_counter=token_counter,
            max_tokens_before_summary=max_tokens_before_summary,
            max_summary_tokens=max_summary_tokens,
            keep_recent_messages=keep_recent_messages,
            prompt_loader=prompt_loader,
        )

    async def __call__(self, state: dict[str, Any]) -> dict[str, Any]:
        """ë…¸ë“œ ë˜ëŠ” hookìœ¼ë¡œ í˜¸ì¶œ."""
        return await self._hook(state)
```

---

## 4. Factory í†µí•©

### 4.1 create_chat_graph íŒŒë¼ë¯¸í„°

```python
def create_chat_graph(
    llm: "LLMClientPort",
    retriever: "RetrieverPort",
    event_publisher: "ProgressNotifierPort",
    prompt_loader: "PromptLoaderPort",
    # ... ê¸°ì¡´ íŒŒë¼ë¯¸í„° ...
    enable_summarization: bool = False,  # LangGraph 1.0+ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
    max_tokens_before_summary: int = 3072,  # ìš”ì•½ íŠ¸ë¦¬ê±° ì„ê³„ê°’
) -> StateGraph:
```

### 4.2 ë…¸ë“œ ë“±ë¡ ë° ì—£ì§€ ì—°ê²°

```python
# ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë…¸ë“œ (ì„ íƒ)
if enable_summarization:
    summarization_node = SummarizationNode(
        llm=llm,
        max_tokens_before_summary=max_tokens_before_summary,
        prompt_loader=prompt_loader,
    )
    logger.info(
        "Summarization enabled (threshold=%d tokens)",
        max_tokens_before_summary,
    )
else:
    summarization_node = None

# ...

# Summarization ë…¸ë“œ ë“±ë¡ (ì„ íƒ)
if summarization_node is not None:
    graph.add_node("summarize", summarization_node)
    logger.info("Summarization node registered")

# ìµœì¢… ëª©ì ì§€ ê²°ì •
final_target = "summarize" if summarization_node is not None else "answer"

# ì—£ì§€ ì—°ê²°
for node_name in ["character", "location", "web_search", "general"]:
    graph.add_edge(node_name, final_target)

if summarization_node is not None:
    graph.add_edge("summarize", "answer")

graph.add_edge("answer", END)
```

---

## 5. ì••ì¶• ì•Œê³ ë¦¬ì¦˜

### 5.1 ì••ì¶• íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Context Compression Flow                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [messages: 20ê°œ, ~5000 tokens]                                  â”‚
â”‚              â”‚                                                   â”‚
â”‚              â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  token_count(messages) > threshold (3072)?          â”‚        â”‚
â”‚  â”‚              â”‚ YES                                   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                 â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Split messages                                      â”‚        â”‚
â”‚  â”‚  â”œâ”€â”€ recent_messages: messages[-4:]  (ìµœê·¼ 4ê°œ)     â”‚        â”‚
â”‚  â”‚  â””â”€â”€ older_messages: messages[:-4]   (ë‚˜ë¨¸ì§€ 16ê°œ)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  LLM.generate(summarize_prompt)                      â”‚        â”‚
â”‚  â”‚  â””â”€â”€ older_messages â†’ summary (~512 tokens)          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Result: [SystemMessage(summary)] + recent_messages  â”‚        â”‚
â”‚  â”‚  â””â”€â”€ ~1500 tokens (70% ì••ì¶•)                         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 ìš”ì•½ í˜•ì‹

```python
# ìš”ì•½ì„ SystemMessageë¡œ ë³€í™˜
summarized_messages = []
if new_summary:
    summarized_messages.append(
        SystemMessage(content=f"[ì´ì „ ëŒ€í™” ìš”ì•½]\n{new_summary}")
    )
summarized_messages.extend(recent_messages)
```

### 5.3 ë¡œê¹… (Observability)

```python
logger.info(
    "context_compressed",
    extra={
        "original_tokens": current_tokens,
        "compressed_tokens": compressed_tokens,
        "compression_ratio": f"{(1 - compressed_tokens / current_tokens) * 100:.1f}%",
    },
)
```

---

## 6. ì‚¬ìš© ì˜ˆì‹œ

### 6.1 Factory í˜¸ì¶œ

```python
# setup/dependencies.py

def get_chat_graph() -> StateGraph:
    return create_chat_graph(
        llm=get_llm_client(),
        retriever=get_retriever(),
        event_publisher=get_progress_notifier(),
        prompt_loader=get_prompt_loader(),
        checkpointer=get_postgres_checkpointer(),  # ë©€í‹°í„´ í•„ìˆ˜
        enable_summarization=True,  # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• í™œì„±í™”
        max_tokens_before_summary=3072,
    )
```

### 6.2 ì²´í¬í¬ì¸í„°ì™€ ì¡°í•©

```python
# ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ì²´í¬í¬ì¸í„° + ì••ì¶• ì¡°í•©
graph = create_chat_graph(
    llm=llm,
    retriever=retriever,
    event_publisher=event_publisher,
    prompt_loader=prompt_loader,
    checkpointer=create_postgres_checkpointer(conn_string),  # ì„¸ì…˜ ìœ ì§€
    enable_summarization=True,  # í† í° ìµœì í™”
    max_tokens_before_summary=3072,
)

# thread_idë¡œ ì„¸ì…˜ ìœ ì§€
config = {"configurable": {"thread_id": user_session_id}}
result = await graph.ainvoke(state, config=config)
```

---

## 7. Clean Architecture ì¤€ìˆ˜

### 7.1 ì˜ì¡´ì„± ë°©í–¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dependency Direction                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Application Layer                                              â”‚
â”‚   â”œâ”€â”€ ports/llm/llm_client.py      â†’ LLMClientPort (ABC)        â”‚
â”‚   â””â”€â”€ ports/prompt_loader.py       â†’ PromptLoaderPort (ABC)     â”‚
â”‚                                                                  â”‚
â”‚                           â–²                                      â”‚
â”‚                           â”‚ implements                           â”‚
â”‚                           â”‚                                      â”‚
â”‚   Infrastructure Layer                                           â”‚
â”‚   â”œâ”€â”€ orchestration/langgraph/summarization.py                  â”‚
â”‚   â”‚   â””â”€â”€ SummarizationNode (uses Ports)                        â”‚
â”‚   â””â”€â”€ assets/prompt_loader.py                                   â”‚
â”‚       â””â”€â”€ FilePromptLoader (implements PromptLoaderPort)        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Port ì‚¬ìš© íŒ¨í„´

| Component | Port | Adapter |
|-----------|------|---------|
| LLM í˜¸ì¶œ | `LLMClientPort` | `GeminiClient`, `OpenAIClient` |
| í”„ë¡¬í”„íŠ¸ ë¡œë”© | `PromptLoaderPort` | `FilePromptLoader` |
| í† í° ì¹´ìš´íŒ… | í•¨ìˆ˜ íŒŒë¼ë¯¸í„° (DI) | `count_tokens_approximately` |

---

## 8. ë³€ê²½ íŒŒì¼ ëª©ë¡

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `infrastructure/orchestration/langgraph/state.py` | ChatState TypedDict, add_messages Reducer |
| `infrastructure/orchestration/langgraph/summarization.py` | SummarizationNode, create_summarization_hook |
| `infrastructure/orchestration/langgraph/factory.py` | enable_summarization íŒŒë¼ë¯¸í„°, ë…¸ë“œ í†µí•© |
| `infrastructure/orchestration/langgraph/__init__.py` | docstring ì—…ë°ì´íŠ¸ |
| `infrastructure/assets/prompts/summarization/context_compress.txt` | ìš”ì•½ í”„ë¡¬í”„íŠ¸ |
| `requirements.txt` | langgraph>=1.0.6, langgraph-checkpoint-redis |

---

## 9. ì°¸ê³  ë¬¸í—Œ

### LangGraph ê³µì‹ ë¬¸ì„œ
- [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/) (2025)
- [LangGraph Message History](https://langchain-ai.github.io/langgraph/concepts/memory/#message-history)
- [Checkpointers](https://langchain-ai.github.io/langgraph/concepts/persistence/)

### ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
- [langmem: SummarizationNode](https://github.com/langchain-ai/langmem)
- [LangGraph Checkpoint Postgres](https://github.com/langchain-ai/langgraph-checkpoint-postgres)

### Anthropic ê¸°ìˆ  ë¬¸ì„œ
- [Effective context engineering for AI agents](https://www.anthropic.com/engineering/effective-context-engineering) (2025)

---

*ë¬¸ì„œ ë²„ì „: v1.0*
*ìµœì¢… ìˆ˜ì •: 2026-01-16*
